{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e003a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# Download the English model\n",
    "stanfordnlp.download('en')\n",
    "\n",
    "# Initialize the NLP pipeline\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,ner')\n",
    "\n",
    "# Sample news article text\n",
    "news_article = \"According to Apple's CEO, Tim Cook, the company plans to release new products this year.\"\n",
    "\n",
    "# Process the news article with NER (Named Entity Recognition)\n",
    "doc = nlp(news_article)\n",
    "\n",
    "# Extract named entities (references)\n",
    "references = []\n",
    "for sent in doc.sentences:\n",
    "    for entity in sent.ents:\n",
    "        references.append(entity.text)\n",
    "\n",
    "print(\"References:\", references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a76013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample data: list of news articles and their corresponding labels (1 for objective, 0 for not)\n",
    "news_articles = [\n",
    "    \"According to Apple's CEO, Tim Cook, the company plans to release new products this year.\",\n",
    "    \"Scientists have discovered a new species of bird in the Amazon rainforest.\",\n",
    "    \"Get rich quick with our amazing investment opportunity!\"\n",
    "]\n",
    "labels = [1, 1, 0]\n",
    "\n",
    "# Initialize the feature extractor (CountVectorizer)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Vectorize the news articles\n",
    "X = vectorizer.fit_transform(news_articles)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd32227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample news titles\n",
    "news_titles = [\n",
    "    \"Stock Market Soars to Record Highs\",\n",
    "    \"Investors Wary of Economic Uncertainty\",\n",
    "    \"Tech Company Reports Strong Q2 Earnings\",\n",
    "    \"Breaking: Fake News Affects Market Sentiment\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(news_titles)\n",
    "\n",
    "# Convert sparse matrix to dense matrix\n",
    "dense_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the TF-IDF vectors and feature names\n",
    "for i, title in enumerate(news_titles):\n",
    "    print(\"News Title:\", title)\n",
    "    print(\"TF-IDF Vector:\", dense_matrix[i])\n",
    "    print(\"Feature Names:\", feature_names)\n",
    "    print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e99dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mock dataset (news_title, news_title, similarity_label)\n",
    "dataset = [\n",
    "    (\"Stock Market Soars to Record Highs\", \"Dow Jones Hits All-Time High\", 1),\n",
    "    (\"Investors Wary of Economic Uncertainty\", \"Tech Stocks Surge Despite Uncertainty\", 0),\n",
    "    (\"Tech Company Reports Strong Q2 Earnings\", \"Apple's Earnings Report Impresses Investors\", 1),\n",
    "    (\"Breaking: Fake News Affects Market Sentiment\", \"Markets React to False Reports\", 0),\n",
    "]\n",
    "\n",
    "# Create training data\n",
    "train_data = []\n",
    "labels = []\n",
    "\n",
    "for item in dataset:\n",
    "    train_data.append(item[0])\n",
    "    train_data.append(item[1])\n",
    "    labels.append(item[2])\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize and pad the news titles\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "sequences = tokenizer.texts_to_sequences(train_data)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Siamese Network architecture\n",
    "input_layer = Input(shape=(padded_sequences.shape[1],))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50)(input_layer)\n",
    "flattened_layer = Flatten()(embedding_layer)\n",
    "dense_layer = Dense(128, activation='relu')(flattened_layer)\n",
    "\n",
    "# Siamese model\n",
    "siamese_model = Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "# Create the left and right input branches\n",
    "input_left = Input(shape=(padded_sequences.shape[1],))\n",
    "input_right = Input(shape=(padded_sequences.shape[1],))\n",
    "\n",
    "# Connect both inputs to the siamese model\n",
    "output_left = siamese_model(input_left)\n",
    "output_right = siamese_model(input_right)\n",
    "\n",
    "# Calculate L1 distance between outputs\n",
    "distance = Lambda(lambda x: tf.abs(x[0] - x[1]))([output_left, output_right])\n",
    "\n",
    "# Final prediction layer\n",
    "prediction = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "# Create the siamese network model\n",
    "siamese_network = Model(inputs=[input_left, input_right], outputs=prediction)\n",
    "\n",
    "# Compile the model\n",
    "siamese_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "siamese_network.fit([padded_sequences[::2], padded_sequences[1::2]], labels, epochs=10, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mock dataset (news_title1, news_title2, similarity_label)\n",
    "dataset = [\n",
    "    (\"Stock Market Soars to Record Highs\", \"Dow Jones Hits All-Time High\", 1),\n",
    "    (\"Investors Wary of Economic Uncertainty\", \"Tech Stocks Surge Despite Uncertainty\", 0),\n",
    "    (\"Tech Company Reports Strong Q2 Earnings\", \"Apple's Earnings Report Impresses Investors\", 1),\n",
    "    (\"Breaking: Fake News Affects Market Sentiment\", \"Markets React to False Reports\", 0),\n",
    "]\n",
    "\n",
    "# Create training data\n",
    "train_data = []\n",
    "labels = []\n",
    "\n",
    "for item in dataset:\n",
    "    train_data.append((item[0], item[1]))  # Adding both news titles\n",
    "    labels.append(item[2])\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.arange(len(train_data))\n",
    "np.random.shuffle(indices)\n",
    "train_data = [train_data[i] for i in indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(train_data) * split_ratio)\n",
    "train_pairs = train_data[:split_idx]\n",
    "train_labels = labels[:split_idx]\n",
    "val_pairs = train_data[split_idx:]\n",
    "val_labels = labels[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample news titles\n",
    "news_titles = [\n",
    "    \"Stock Market Soars to Record Highs\",\n",
    "    \"Dow Jones Hits All-Time High\",\n",
    "    \"Investors Wary of Economic Uncertainty\",\n",
    "    \"Tech Stocks Surge Despite Uncertainty\"\n",
    "]\n",
    "\n",
    "# Preprocessing and tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Tokenize and preprocess news titles\n",
    "preprocessed_titles = [preprocess_text(title) for title in news_titles]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=preprocessed_titles, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Convert news titles to word embeddings\n",
    "title_embeddings = [model.wv[title] for title in preprocessed_titles]\n",
    "\n",
    "# Display word embeddings\n",
    "for i, title in enumerate(news_titles):\n",
    "    print(f\"News Title: {title}\")\n",
    "    print(f\"Word Embedding: {title_embeddings[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d074db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the news website\n",
    "url = \"https://www.example-news-website.com\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news article links on the page\n",
    "article_links = soup.find_all(\"a\", class_=\"article-link\")\n",
    "\n",
    "# Loop through the article links and extract relevant information\n",
    "for link in article_links:\n",
    "    article_url = link[\"href\"]\n",
    "    article_title = link.text\n",
    "\n",
    "    # Visit the article URL and scrape the content\n",
    "    article_response = requests.get(article_url)\n",
    "    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the article content\n",
    "    article_content = article_soup.find(\"div\", class_=\"article-content\").get_text()\n",
    "\n",
    "    # Perform additional processing on the article content\n",
    "    # Extract mentions of companies, numbers, etc.\n",
    "\n",
    "    print(\"Title:\", article_title)\n",
    "    print(\"URL:\", article_url)\n",
    "    print(\"Content:\", article_content)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"The stock market gained 100 points yesterday, while Company XYZ's shares increased by $5.50.\"\n",
    "\n",
    "# Regular expression pattern to match numbers with stock market indicators\n",
    "pattern = r'\\b(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:points|shares|\\$)\\b'\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "# Print the extracted numbers\n",
    "for match in matches:\n",
    "    print(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501be1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numbers_with_indicators(text):\n",
    "    # Regular expression pattern to match numbers with stock market indicators\n",
    "    pattern = r'\\b(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:points|shares|\\$)\\b'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "text = \"The stock market gained 100 points yesterday, while Company XYZ's shares increased by $5.50.\"\n",
    "numbers_with_indicators = extract_numbers_with_indicators(text)\n",
    "\n",
    "# Print the extracted numbers with indicators\n",
    "for number in numbers_with_indicators:\n",
    "    print(number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_author_name(author_name):\n",
    "    # Remove spaces and convert to lowercase\n",
    "    normalized_name = author_name.replace(\" \", \"\").lower()\n",
    "    return normalized_name\n",
    "\n",
    "def normalize_website_url(website_url):\n",
    "    # Remove protocol and www subdomain, then lowercase\n",
    "    normalized_url = re.sub(r'(https?://)?(www\\.)?', '', website_url, flags=re.IGNORECASE)\n",
    "    normalized_url = normalized_url.lower()\n",
    "    return normalized_url\n",
    "\n",
    "author_name = \"John Doe\"\n",
    "website_url = \"https://www.Example.com\"\n",
    "\n",
    "normalized_author = normalize_author_name(author_name)\n",
    "normalized_url = normalize_website_url(website_url)\n",
    "\n",
    "print(\"Normalized Author:\", normalized_author)\n",
    "print(\"Normalized URL:\", normalized_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09152a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f02759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438bf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c9176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffbf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
